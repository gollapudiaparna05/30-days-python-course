{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9149d332-9234-49c6-a379-6fda32f12bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#day19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef147617-0e8d-47a9-af46-63a6b796ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama_speech.txt: 66 lines, 2401 words\n",
      "michelle_obama_speech.txt: 83 lines, 2223 words\n",
      "donald_speech.txt: 48 lines, 1266 words\n",
      "melina_trump_speech.txt: 33 lines, 1377 words\n",
      "\n",
      "Most spoken languages (top 10):\n",
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 9), ('Russian', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Serbian', 4)]\n",
      "\n",
      "Most spoken languages (top 3):\n",
      "[('English', 91), ('French', 45), ('Arabic', 25)]\n",
      "\n",
      "Most populated countries (top 10):\n",
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}, {'country': 'Indonesia', 'population': 258705000}, {'country': 'Brazil', 'population': 206135893}, {'country': 'Pakistan', 'population': 194125062}, {'country': 'Nigeria', 'population': 186988000}, {'country': 'Bangladesh', 'population': 161006790}, {'country': 'Russian Federation', 'population': 146599183}, {'country': 'Japan', 'population': 126960000}]\n",
      "\n",
      "Most populated countries (top 3):\n",
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}]\n"
     ]
    }
   ],
   "source": [
    "# Day 19: 30 Days of Python - File Handling and Text Analysis\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# -----------------------------\n",
    "#  Exercise Level 1\n",
    "# -----------------------------\n",
    "\n",
    "# Count number of lines and words in a text file\n",
    "def count_lines_and_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        lines = text.splitlines()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return len(lines), len(words)\n",
    "\n",
    "\n",
    "files = [\n",
    "    'obama_speech.txt',\n",
    "    'michelle_obama_speech.txt',\n",
    "    'donald_speech.txt',\n",
    "    'melina_trump_speech.txt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    lines, words = count_lines_and_words(file)\n",
    "    print(f\"{file}: {lines} lines, {words} words\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Most spoken languages\n",
    "# -----------------------------\n",
    "\n",
    "def most_spoken_languages(filename, top_n):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        countries = json.load(f)\n",
    "    languages = []\n",
    "    for country in countries:\n",
    "        languages.extend(country['languages'])\n",
    "    lang_counts = Counter(languages)\n",
    "    return lang_counts.most_common(top_n)\n",
    "\n",
    "\n",
    "print(\"\\nMost spoken languages (top 10):\")\n",
    "print(most_spoken_languages('countries_data.json', 10))\n",
    "\n",
    "print(\"\\nMost spoken languages (top 3):\")\n",
    "print(most_spoken_languages('countries_data.json', 3))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Most populated countries\n",
    "# -----------------------------\n",
    "\n",
    "def most_populated_countries(filename, top_n):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        countries = json.load(f)\n",
    "    sorted_countries = sorted(countries, key=lambda x: x['population'], reverse=True)\n",
    "    top_countries = [{'country': c['name'], 'population': c['population']} for c in sorted_countries[:top_n]]\n",
    "    return top_countries\n",
    "\n",
    "\n",
    "print(\"\\nMost populated countries (top 10):\")\n",
    "print(most_populated_countries('countries_data.json', 10))\n",
    "\n",
    "print(\"\\nMost populated countries (top 3):\")\n",
    "print(most_populated_countries('countries_data.json', 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf9c878-c199-4e29-978e-f59c34fb849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted emails: ['stephen.marquard@uct.ac.za', 'postmaster@collab.sakaiproject.org', '200801051412.m05ECIaH010327@nakamura.uits.iupui.edu', 'source@collab.sakaiproject.org', 'source@collab.sakaiproject.org', 'source@collab.sakaiproject.org', 'source@collab.sakaiproject.org', 'stephen.marquard@uct.ac.za', 'source@collab.sakaiproject.org', 'stephen.marquard@uct.ac.za'] ...\n",
      "Total emails found: 20140\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Extract all email addresses\n",
    "# -----------------------------\n",
    "\n",
    "def extract_emails(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return re.findall(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', text)\n",
    "\n",
    "\n",
    "emails = extract_emails('email_exchanges_big.txt')\n",
    "print(\"\\nExtracted emails:\", emails[:10], '...')  # show first 10\n",
    "print(\"Total emails found:\", len(emails))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b834426b-21b2-4ac5-bcee-1db577389ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obama’s top 10 words: [('the', 129), ('and', 113), ('of', 81), ('to', 70), ('our', 67), ('we', 62), ('that', 50), ('a', 48), ('is', 36), ('in', 25)]\n",
      "Michelle’s top 10 words: [('and', 96), ('the', 85), ('to', 84), ('that', 50), ('of', 46), ('a', 41), ('he', 37), ('in', 36), ('my', 28), ('i', 28)]\n",
      "Trump’s top 10 words: [('the', 65), ('and', 59), ('we', 44), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('is', 20), ('america', 17), ('for', 13)]\n",
      "Melina’s top 10 words: [('and', 77), ('to', 55), ('the', 52), ('is', 29), ('i', 28), ('for', 27), ('of', 25), ('that', 24), ('a', 22), ('you', 21)]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Most frequent words in speeches\n",
    "# -----------------------------\n",
    "\n",
    "def find_most_common_words(filename, n):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return Counter(words).most_common(n)\n",
    "\n",
    "\n",
    "def most_frequent_words(filename, n=10):\n",
    "    return find_most_common_words(filename, n)\n",
    "\n",
    "print(\"\\nObama’s top 10 words:\", most_frequent_words('obama_speech.txt'))\n",
    "print(\"Michelle’s top 10 words:\", most_frequent_words('michelle_obama_speech.txt'))\n",
    "print(\"Trump’s top 10 words:\", most_frequent_words('donald_speech.txt'))\n",
    "print(\"Melina’s top 10 words:\", most_frequent_words('melina_trump_speech.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73dd01c0-4978-4b2c-81f4-97708688d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text similarity between Michelle and Melina speeches: 0.72%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Text similarity\n",
    "# -----------------------------\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z\\s]', '', text).lower()\n",
    "\n",
    "def check_text_similarity(file1, file2):\n",
    "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "        text1, text2 = clean_text(f1.read()), clean_text(f2.read())\n",
    "    similarity = SequenceMatcher(None, text1, text2).ratio()\n",
    "    return round(similarity * 100, 2)\n",
    "\n",
    "\n",
    "similarity_score = check_text_similarity('michelle_obama_speech.txt', 'melina_trump_speech.txt')\n",
    "print(f\"\\nText similarity between Michelle and Melina speeches: {similarity_score}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2909ccad-e224-4776-9799-4cabc365e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in Romeo and Juliet:\n",
      "[('the', 868), ('and', 800), ('to', 661), ('i', 658), ('of', 535), ('a', 530), ('is', 381), ('in', 378), ('that', 371), ('you', 367)]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Top repeated words in Romeo and Juliet\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nTop 10 words in Romeo and Juliet:\")\n",
    "print(find_most_common_words('romeo_and_juliet.txt', 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e40634-35f5-40e1-8771-37f2dded5324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lines containing 'Python': 162\n",
      "Lines containing 'JavaScript': 183\n",
      "Lines containing 'Java' but not 'JavaScript': 59\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hacker News CSV word frequency\n",
    "# -----------------------------\n",
    "\n",
    "def count_keyword_occurrences(filename, keyword):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return sum(1 for line in lines if re.search(keyword, line, re.IGNORECASE))\n",
    "\n",
    "python_count = count_keyword_occurrences('hacker_news.csv', r'\\bpython\\b')\n",
    "js_count = count_keyword_occurrences('hacker_news.csv', r'\\bjavascript\\b')\n",
    "java_count = count_keyword_occurrences('hacker_news.csv', r'\\bjava\\b(?!script)')\n",
    "\n",
    "print(\"\\nLines containing 'Python':\", python_count)\n",
    "print(\"Lines containing 'JavaScript':\", js_count)\n",
    "print(\"Lines containing 'Java' but not 'JavaScript':\", java_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fdb8b-0ae9-4c37-b0a6-aa56aa2954c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
